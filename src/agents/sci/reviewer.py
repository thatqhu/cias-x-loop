"""
Plan Reviewer Agent - Experiment Plan Validator

Reviewer agent that validates, critiques, and approves/rejects experiment plans
generated by the Planner Agent. Ensures safety, validity, and alignment with goals.
"""

from typing import List, Dict, Any, Tuple, Optional
from loguru import logger
import json

from .structures import SCIConfiguration, ReviewResult
from ...llm.client import LLMClient
from ...agents.utils import Utils
from ...llm.client import LLMClient
from .world_model import WorldModel

from ..base import BaseAgent


class PlanReviewerAgent(BaseAgent):
    """Agent for reviewing and validating experiment plans"""

    def __init__(self, design_space: Dict[str, Any], llm_client: LLMClient, world_model: WorldModel):
        """
        Initialize Reviewer Agent

        Args:
            design_space: Valid design space for rule-based verification
            llm_client: LLM client for semantic review
            world_model: World Model instance
        """
        super().__init__("ReviewerAgent", llm_client, world_model)

        self.design_space = design_space
        logger.info("Plan Reviewer Agent initialized")

    async def review_plan(
        self,
        proposed_configs: List[SCIConfiguration],
        context: Dict[str, Any]
    ) -> ReviewResult:
        """
        Review a list of proposed experiment configurations

        Args:
            proposed_configs: List of configurations to review
            context: Context dictionary (cycle info, current bests, etc.)

        Returns:
            ReviewResult containing verdict and feedback
        """
        logger.info(f"Reviewing plan with {len(proposed_configs)} configs")

        # Stage 1: Fast Rule-based Verification (Sanity Checks)
        valid_configs = []
        rule_rejections = {}

        for config in proposed_configs:
            is_valid, reason = self._check_rules(config)
            if is_valid:
                valid_configs.append(config)
            else:
                rule_rejections[config.experiment_id] = f"Rule Violation: {reason}"
                logger.warning(f"Config {config.experiment_id} rejected by rules: {reason}")

        if not valid_configs:
            return ReviewResult(
                approved=False,
                approved_configs=[],
                feedback="All configurations failed basic sanity checks.",
                critique=rule_rejections
            )

        # Stage 2: LLM-based Strategic Review
        try:
            llm_result = await self._llm_review(valid_configs, context)

            # Merge rule rejections into final critique
            final_critique = {**rule_rejections, **llm_result['critique']}

            # Filter configs based on LLM approval
            final_approved_configs = []
            for config in valid_configs:
                if config.experiment_id in llm_result['approved_ids']:
                    final_approved_configs.append(config)
                else:
                    logger.info(f"Config {config.experiment_id} rejected by LLM: {final_critique.get(config.experiment_id)}")

            is_approved = len(final_approved_configs) > 0

            return ReviewResult(
                approved=is_approved,
                approved_configs=final_approved_configs,
                feedback=llm_result['feedback'],
                critique=final_critique
            )

        except Exception as e:
            logger.error(f"LLM review failed: {e}")
            # Fallback: if LLM fails, trust the rule-based check but warn
            return ReviewResult(
                approved=True,
                approved_configs=valid_configs,
                feedback=f"LLM review failed ({str(e)}), proceeding with rule-compliant configs.",
                critique=rule_rejections
            )

    def _check_rules(self, config: SCIConfiguration) -> Tuple[bool, str]:
        """
        Perform strict rule-based sanity checks

        Returns:
            (valid, reason)
        """
        # 1. Check Compression Ratio
        valid_crs = self.design_space.get('compression_ratios', [])
        if config.forward_config.compression_ratio not in valid_crs:
            return False, f"Invalid compression_ratio {config.forward_config.compression_ratio}. Allowed: {valid_crs}"

        # 2. Check Resource Constraints (Simple Heuristic)
        # Example: Don't allow high stages + high features + high epochs
        stages = config.recon_params.num_stages
        features = config.recon_params.num_features
        epochs = config.train_config.num_epochs

        complexity_score = stages * features * epochs
        MAX_COMPLEXITY = 9 * 128 * 100  # Arbitrary limit based on max values

        if complexity_score > MAX_COMPLEXITY:
            return False, f"Estimated complexity {complexity_score} exceeds limit {MAX_COMPLEXITY}"

        # 3. Check Learning Rate safety
        if config.recon_params.learning_rate > 1e-2:
            return False, f"Learning rate {config.recon_params.learning_rate} is suspiciously high (>0.01)"

        return True, ""

    async def _llm_review(
        self,
        configs: List[SCIConfiguration],
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Ask LLM to review the scientific validity and strategy
        """
        prompt = self._build_review_prompt(configs, context)

        messages = [
            {"role": "system", "content": """You are a Senior Reviewer for scientific experiments in Snapshot Compressive Imaging.
Your job is to strictly evaluate proposed experiment configurations for safety, validity, and scientific value.
You are skeptical and prioritize resource efficiency."""},
            {"role": "user", "content": prompt}
        ]

        response = self.llm_client.chat(messages, response_format="json")

        try:
            content = Utils.extract_json_from_response(response['content'])
            result = json.loads(content)
            return result
        except Exception as e:
            logger.error(f"Failed to parse LLM review response: {e}")
            # If parse fails, assume approval to avoid blocking, but log error
            return {
                "approved_ids": [c.experiment_id for c in configs],
                "feedback": "Review parser failed, defaulting to approval.",
                "critique": {}
            }

    def _build_review_prompt(self, configs: List[SCIConfiguration], context: Dict[str, Any]) -> str:
        """Build the prompt for the LLM reviewer"""

        # Format the configs for the prompt
        config_list_str = ""
        for cfg in configs:
            config_list_str += f"""
Experiment ID: {cfg.experiment_id}
- Compression: {cfg.forward_config.compression_ratio}
- Stages: {cfg.recon_params.num_stages}
- Features: {cfg.recon_params.num_features}
- LR: {cfg.recon_params.learning_rate}
- Activation: {cfg.recon_params.activation}
----------------------------------------"""

        # Format context
        cycle = context.get('cycle', '?')
        best_psnr = context.get('best_psnr', 'N/A')

        return f"""
**Review Task**
Please review the following proposed experiments for Cycle {cycle}.
Current Best PSNR: {best_psnr} dB.

**Proposed Experiments:**
{config_list_str}

**Criteria:**
1. **Safety**: Reject parameters that are likely to diverge or crash (e.g. extremely high LR with deep networks).
2. **Redundancy**: Reject experiments that look identical to previous failures (if info available).
3. **Strategy**:
   - If early cycle (1-2), favor Exploration (diverse params).
   - If late cycle (4+), favor Exploitation (refining best params).

**Output Format:**
Return valid JSON:
{{
    "approved_ids": ["exp_1", ...],
    "feedback": "General feedback on the batch strategy...",
    "critique": {{
        "exp_ID_rejected": "Reason for rejection...",
        "exp_ID_approved": "Good because..."
    }}
}}
"""
